{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import re as re\n",
    "\n",
    "from pyspark.ml.pipeline import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.ml.util import keyword_only\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, MapType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "\n",
    "__author__ = \"Su-Young Hong AKA Da Masta Killa AKA Synth Pop Rocks of Locks AKA Intergalactic Chilympian\"\n",
    "__status__ = \"Prototype\"\n",
    "\n",
    "# read directory of files and return a list of all driverIDs from csv's insid directory\n",
    "def get_drivers(dirpath):\n",
    "    \"\"\"\n",
    "    :param dirpath: string, path to directory containing driver csv's\n",
    "    :return: list, contains all driverIDs as strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        allfiles = os.listdir(dirpath)\n",
    "        drivers = [re.sub(r'[^0-9]', '', i) for i in allfiles]\n",
    "        drivers.remove('')\n",
    "        return drivers\n",
    "    except Exception as e:\n",
    "        print e\n",
    "\n",
    "# produces random samples of driverIDs and tripIDs in two separate lists\n",
    "def random_samples(targ_driv, driv_list, K=200):\n",
    "    \"\"\"\n",
    "    :param targ_driv: str, driverID we want to make false trips for\n",
    "    :param driv_list: list, list of all drivers, produced by get_drivers()\n",
    "    :param K: number of trips we want to make for targ_driv\n",
    "    :return: tuple of lists, first list is random driverIDs, second list is list of tripIDs, both are strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driv_list.remove(targ_driv) #removes the target driver from list of drivers to sample from\n",
    "        drivers = np.random.choice(driv_list, K, True)\n",
    "        trips = np.random.choice(np.arange(1,K+1).astype(str), K, True)\n",
    "        return (drivers, trips)\n",
    "    except Exception as e:\n",
    "        print e\n",
    "\n",
    "# reads directory of files and returns RDD of observations from trips in the sample (driverID, tripID combo)\n",
    "# NOTE: this function is VERY SLOW, it is what slows the entire workflow down\n",
    "def sample_data(path, driverIDs, tripIDs):\n",
    "    \"\"\"\n",
    "    :param path: string, path to directory containing driver.csv's\n",
    "    :param driverIDs: list, list of randomly sampled driverIDs as strings, produced by random_sample()\n",
    "    :param tripIDs: list, list of randomly sampled tripIDs as strings, produced by random_samples()\n",
    "        NOTE: the above two zip into a list of (driverID, tripID) tuples, with each tuple being a single item in the\n",
    "        sample\n",
    "    :return: RDD, contains only observations from the sample\n",
    "    \"\"\"\n",
    "    try:\n",
    "        combos = zip(driverIDs, tripIDs)\n",
    "        samplefiles = [path + '/' + 'driver_' + i + '.csv' for i in driverIDs]\n",
    "        samplefiles = ','.join(set(samplefiles))  #### NOTE: this set() action is a hack for small num. files\n",
    "        RDD = sc.textFile(samplefiles)   #### NOTE: with large num. files, might need to set num. partitions\n",
    "        RDDsplit = RDD.map(lambda x: x.split(','))\n",
    "        RDDsamples = RDDsplit.filter(lambda x: (x[2],x[3]) in combos)\n",
    "        RDDsamples.cache()\n",
    "        return RDDsamples\n",
    "    except Exception as e:\n",
    "        print e\n",
    "\n",
    "# takes RDD of samples and assigns new driverID and tripID to observations in a new RDD\n",
    "def ID_Data(targ_driver, RDD, K = 200):\n",
    "    \"\"\"\n",
    "    :param targ_driver: string, target driver we used to generate samples\n",
    "    :param RDD: RDD, trip data RDD produced by sample_data(), format will be original form (x,y,driverID,tripID,step)\n",
    "    :param K: int, number of trips we sampled\n",
    "    :return: RDD, in original format, but with driverID and tripID changed to look like new observations of the target\n",
    "    driver\n",
    "    \"\"\"\n",
    "    try:\n",
    "        newID1 = [targ_driver] * K\n",
    "        newID2 = np.arange(200, 201+K).astype(str)\n",
    "        newID = zip(newID1, newID2)\n",
    "        oldID = RDD.map(lambda x: (x[2],x[3])).distinct().collect()\n",
    "        glossary = sc.parallelize(zip(oldID, newID))\n",
    "        newRDD = RDD.map(lambda x: ((x[2],x[3]), ([x[0],x[1],x[4]]))).join(glossary)\n",
    "        newID_RDD = newRDD.map(lambda x: (x[1][0][0], x[1][0][1], x[1][1][0], x[1][1][1], x[1][0][2]))\n",
    "        return newID_RDD\n",
    "    except Exception as e:\n",
    "        print e\n",
    "\n",
    "\n",
    "# takes RDD in original form and converts it into key-value tuple with values being x,y,step,label\n",
    "def processRDD(RDD, label):\n",
    "    \"\"\"\n",
    "    :param RDD: RDD in original format (x,y,driverID,tripID,step)\n",
    "    :param label: category of observation, 1 for positive, 0 for negative\n",
    "    # note, not sure if it needs to be int or float\n",
    "    :return: RDD, RDD returned in new key/value format: (driverID, tripID), (x, y, step, label)\n",
    "    # note, x, y, step, and label will be floats\n",
    "    \"\"\"\n",
    "    try:\n",
    "        newRDD = RDD.map(lambda x: ((x[2],x[3]),(float(x[0]),float(x[1]),float(x[4]),label)))\n",
    "        return newRDD\n",
    "    except Exception as e:\n",
    "        print e\n",
    "\n",
    "# takes a driver to target, path to directory of driver.csv's, and returns an RDD labeled with\n",
    "# (driverID, tripID),(x,y,step,label), where a label 1 is from an actual trip, and label 0 is from\n",
    "# a trip randomly sampled from other drivers\n",
    "def labelRDDs(targ_driv, path, K=200):\n",
    "    \"\"\"\n",
    "    :param targ_driv: string, driver we want to create positive and negative labeled data for\n",
    "    :param path: string, path to directory where driver.csvs are stored\n",
    "    :param K: int, number of negative (manufactured) trips to sample\n",
    "    :return: RDD with key, value tuple where key is (driverID, tripID) and value is (x,y,step,label)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_path = path + '/' + 'driver_' + targ_driv + '.csv'\n",
    "        #print full_path\n",
    "        target = sc.textFile(path + '/' + 'driver_' + targ_driv + '.csv') #load target driver's data\n",
    "        target2 = target.map(lambda x: x.split(',')) #convert from string to list of strings\n",
    "        positives = processRDD(target2, 1.0) #label target driver's RDD\n",
    "        driv_lis = get_drivers(path) #get python list of all possible drivers to sample from\n",
    "        #print driv_lis\n",
    "        sampdriv, samptrip = random_samples(targ_driv, driv_lis, K) #generate random samples of drivers and tripIDs\n",
    "        samples = sample_data(path, sampdriv, samptrip) #generate RDD of random samples\n",
    "        #print \"GETS HERE\"\n",
    "        samplesRDD = ID_Data(targ_driv, samples, K) #relabel samples to look like target driver's trips\n",
    "        #print \"GETS HERE TOO\"\n",
    "        negatives = processRDD(samplesRDD, 0.0) #label samples\n",
    "        finalRDD = positives.union(negatives).cache() #join target driver and samples together\n",
    "        return finalRDD\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tuples(row):\n",
    "    data = row[1]\n",
    "    x = []\n",
    "    y = []\n",
    "    steps = []\n",
    "    label = None\n",
    "    for d in data:\n",
    "        x.append(d[0])\n",
    "        y.append(d[1])\n",
    "        steps.append(d[2])\n",
    "        label = d[3]\n",
    "    return Row(label=label, signature={\"x\":x, \"y\":y, \"steps\":steps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/mayankkedia/code/kaggle/axa_telematics/sample_drivers'\n",
    "driver = '1'\n",
    "d1_RDD = labelRDDs('1', path).groupByKey().map(lambda x: combine_tuples(x))\n",
    "d1_df = sqlContext.createDataFrame(d1_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- signature: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def add_polar_coords(signature):\n",
    "    x = signature[\"x\"]\n",
    "    y = signature[\"y\"]\n",
    "    \n",
    "    r = np.sqrt(np.square(x) + np.square(y)).tolist()\n",
    "    theta = map(lambda p:  math.atan2(p[1],p[0]), zip(x,y))\n",
    "    \n",
    "    return {\"x\":x, \"y\":y, \"r\":r, \"theta\":theta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkUDF = udf(add_polar_coords, MapType(StringType(), ArrayType(FloatType())))\n",
    "d1_df_nc = d1_df.withColumn(\"new_signature\", sparkUDF(\"signature\"))\n",
    "d1_df_n = d1_df_nc.select([\"label\", \"new_signature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- new_signature: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1_df_n.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_step_level_features(new_signature):\n",
    "    x = new_signature[\"x\"]\n",
    "    y = new_signature[\"y\"]\n",
    "    r = new_signature[\"r\"]\n",
    "    theta = new_signature[\"theta\"]  \n",
    "    # x[0] represents the (x[i] - x[i-1])^2 \n",
    "    # x[1] represents (y[i] - y[i-1])^2\n",
    "    v = map(lambda x: (x[0] + x[1]) ** 0.5, \n",
    "                zip(\n",
    "                    # Zipping the x coordinates with itself (one row behind) and then \n",
    "                    # calculating the velocity by calculating distance traveled in time step 1\n",
    "                    map(lambda x: (x[0] - x[1]) ** 2, zip(x, [0.0] + x[:-1])), \n",
    "                    map(lambda x: (x[0] - x[1]) ** 2, zip(y, [0.0] + y[:-1]))\n",
    "                   )\n",
    "               )\n",
    "    a =  map(lambda x: x[0] - x[1],\n",
    "            zip(v, [0.0] + v[:-1]))\n",
    "    \n",
    "    return {\"x\":x, \"y\":y, \"r\":r, \"theta\":theta, \"v\":v, \"a\":a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkUDF = udf(add_step_level_features, MapType(StringType(), ArrayType(FloatType())))\n",
    "d1_df_sl = d1_df_n.withColumn(\"signature\", sparkUDF(\"new_signature\")).select([\"label\", \"signature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- signature: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1_df_sl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|           signature|\n",
      "+-----+--------------------+\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  1.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  1.0|Map(x -> WrappedA...|\n",
      "|  1.0|Map(x -> WrappedA...|\n",
      "|  1.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  1.0|Map(x -> WrappedA...|\n",
      "|  1.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  1.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "|  0.0|Map(x -> WrappedA...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_trip_level_features(signature):\n",
    "    \n",
    "    \n",
    "\n",
    "def get_velocity_percentiles(row):\n",
    "    \"\"\"\n",
    "    Generates the percentiles for 5, 10, 15 ... 95 for Velocity\n",
    "    \"\"\"\n",
    "    v = row[1][4]\n",
    "    return np.percentile(v, range(5, 100, 5))\n",
    "\n",
    "\n",
    "def get_acceleration_percentiles(row):\n",
    "    \"\"\"\n",
    "    Generates the percentiles for 5, 10, 15 ... 95 for Acceleration\n",
    "    \"\"\"\n",
    "    a = row[1][5]\n",
    "    return np.percentile(a, range(5, 100, 5))\n",
    "\n",
    "\n",
    "def trip_features(x):\n",
    "    \"\"\"\n",
    "    Calculates the features of the trip from a row which is of the form\n",
    "    ((driver_id, trip_id), ([x coordinates], [y coordinates],\n",
    "    [r coordinates], [theta coordinates], [v coordinates], [step numbers], label))\n",
    "    This is the form of the rows of the output from step_level_features.\n",
    "\n",
    "    :@param x:\n",
    "    \"\"\"\n",
    "    min_v = min(x[1][4])\n",
    "    max_v = max(x[1][4])\n",
    "    min_a = min(x[1][5])\n",
    "    max_a = max(x[1][5])\n",
    "    trip_length = len(x[1][0])\n",
    "    mean_v = np.mean(x[1][4])\n",
    "    std_v = np.std(x[1][4])\n",
    "    mean_a = np.mean(x[1][5])\n",
    "    std_a = np.std(x[1][5])\n",
    "    time_stop = sum([elem < 0.5 for elem in x[1][4]])\n",
    "    label = x[1][7]\n",
    "\n",
    "    numerical_features = (min_v, max_v,\n",
    "                   min_a, max_a,\n",
    "                   trip_length,\n",
    "                   mean_v, std_v,\n",
    "                   mean_a, std_a,\n",
    "                   time_stop,\n",
    "                   label)\n",
    "\n",
    "    v_percentiles = get_velocity_percentiles(x)\n",
    "    a_percentiles = get_acceleration_percentiles(x)\n",
    "    percentiles = np.append(v_percentiles, a_percentiles)\n",
    "    second_tuple = np.append(percentiles, numerical_features).tolist()\n",
    "    return (x[0], second_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}